{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GcrlIhMbRwOS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "RSPgmWpCSM3N",
    "outputId": "357d493c-8224-4962-eceb-b3e901be1396"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPUFQHp-Tlsj"
   },
   "outputs": [],
   "source": [
    "def load_data(x): # flag 1 for training data, 0 for testing data\n",
    "    drct = r\"../data/TrainData/\"\n",
    "    \n",
    "    imgL = []\n",
    "    label = []\n",
    "    \n",
    "    if x:\n",
    "        df = pd.read_csv('../data/TrainAnnotations.csv')\n",
    "        for i in range(len(df.annotation)):\n",
    "            #print(df.file_name[i])\n",
    "            name = str(df.file_name[i])\n",
    "            path = drct+name\n",
    "            img = cv2.imread(path)\n",
    "            img = cv2.resize(img,(224,224))  \n",
    "            \n",
    "            image = img #[:,:,1]\n",
    "            imgL.append(image) #hog_image_rescaled\n",
    "            \n",
    "            if df.annotation[i] == 0: label.append(0)\n",
    "            elif df.annotation[i] == 1: label.append(1)\n",
    "            elif df.annotation[i] == 2: label.append(2)\n",
    "            elif df.annotation[i] == 3: label.append(3)\n",
    "            elif df.annotation[i] == 4: label.append(4)\n",
    "           \n",
    "        data = np.array(imgL) \n",
    "        \n",
    "        clas = np.array(label)\n",
    "        return data, clas # return five respective image lists\n",
    "\n",
    "    else: \n",
    "        img_dir = \"/content/drive/My Drive/C2/data/TestData\" # Enter Directory of all images \n",
    "        data_path = os.path.join(img_dir,'*g')\n",
    "        files = glob.glob(data_path)\n",
    "        data=[]\n",
    "        data_list = []\n",
    "        for f1 in files:\n",
    "          data_list.append(f1)\n",
    "        data_list = np.array(data_list)\n",
    "        data_list = np.unique(data_list)\n",
    "\n",
    "        for img_path in data_list:\n",
    "          img = cv2.imread(img_path)          \n",
    "          img = cv2.resize(img,(224,224))     #[#100:300,100:300]           \n",
    "          image = img    \n",
    "          data.append(image)#[:,:,1]\n",
    "          \n",
    "        data = np.array(data)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "mPMcoB8DvKLh",
    "outputId": "154ee019-d8f2-4900-e47b-299b8fc6f312"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cV6O4Q_-RwOd"
   },
   "outputs": [],
   "source": [
    "x_train, y_train =  load_data(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IFtczRPlRwOg"
   },
   "outputs": [],
   "source": [
    "def get_dir(x,y):\n",
    "    d_dir = {0:[],1:[],2:[],3:[],4:[]}\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        for c in range(5):\n",
    "            if y[i] == c: \n",
    "                d_dir[c].append(x[i])\n",
    "                break\n",
    "    return d_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aAVgNb0jRwOn"
   },
   "outputs": [],
   "source": [
    "def get_data(dictionary):   # if 1 train if 0 validation\n",
    "    train_data = dictionary[0] + dictionary[1] + dictionary[2] + dictionary[3] + dictionary[4]\n",
    "    \n",
    "    train_label = np.zeros(len(dictionary[0]))\n",
    "    train_label = np.append(train_label, np.ones(len(dictionary[1])))\n",
    "    train_label = np.append(train_label, 2*np.ones(len(dictionary[2])))\n",
    "    train_label = np.append(train_label, 3*np.ones(len(dictionary[3])))\n",
    "    train_label = np.append(train_label, 4*np.ones(len(dictionary[4])))\n",
    "    \n",
    "    data = np.ndarray((len(train_data),224,224,3))\n",
    "    i = 0\n",
    "    for img in train_data:\n",
    "        np.resize(img,(224,224))\n",
    "        data[i] = img\n",
    "        i+=1\n",
    "        \n",
    "    return data, train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "U9J7i0ehRwOs",
    "outputId": "7e5a875b-1bde-48ac-8fed-96a4192b6a95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1931, 224, 224, 3) (1931,) (201, 224, 224, 3) (201,)\n"
     ]
    }
   ],
   "source": [
    "imgL0=[]\n",
    "imgL1=[]\n",
    "imgL2=[]\n",
    "imgL3=[]\n",
    "imgL4=[]\n",
    "\n",
    "cla0=[]\n",
    "cla1=[]\n",
    "cla2=[]\n",
    "cla3=[]\n",
    "cla4=[]\n",
    "\n",
    "val_d0=[]\n",
    "val_c0=[]\n",
    "val_d1=[]\n",
    "val_c1=[]\n",
    "val_d2=[]\n",
    "val_c2=[]\n",
    "val_d3=[]\n",
    "val_c3=[]\n",
    "val_d4=[]\n",
    "val_c4=[]\n",
    "\n",
    "for i in range (0,len(y_train)):\n",
    "  if y_train[i]==0:\n",
    "    L_0=x_train[i]\n",
    "    imgL0.append(L_0)\n",
    "    C_0=y_train[i]\n",
    "    cla0.append(C_0)\n",
    "\n",
    "  elif y_train[i]==1:\n",
    "     L_1=x_train[i]\n",
    "     imgL1.append(L_1)\n",
    "     C_1=y_train[i]\n",
    "     cla1.append(C_1)\n",
    "\n",
    "  elif y_train[i]==2:\n",
    "     L_2=x_train[i]\n",
    "     imgL2.append(L_2)\n",
    "     C_2=y_train[i]\n",
    "     cla2.append(C_2)\n",
    "\n",
    "  elif y_train[i]==3:\n",
    "    L_3=x_train[i]\n",
    "    imgL3.append(L_3)\n",
    "    C_3=y_train[i]\n",
    "    cla3.append(C_3)\n",
    "\n",
    "  elif y_train[i]==4:\n",
    "    L_4=x_train[i]\n",
    "    imgL4.append(L_4)\n",
    "    C_4=y_train[i]\n",
    "    cla4.append(C_4)\n",
    "\n",
    "train_X0, test_X0, train_y0, test_y0 = train_test_split(imgL0,cla0,test_size=0.08)\n",
    "train_X1, test_X1, train_y1, test_y1 = train_test_split(imgL1,cla1,test_size=0.12)\n",
    "train_X2, test_X2, train_y2, test_y2 = train_test_split(imgL2,cla2,test_size=0.31)\n",
    "train_X3, test_X3, train_y3, test_y3 = train_test_split(imgL3,cla3,test_size=0.30)\n",
    "train_X4, test_X4, train_y4, test_y4 = train_test_split(imgL4,cla4,test_size=0.20)\n",
    "\n",
    "train_X0=np.array(train_X0)\n",
    "vald0=np.array(test_X0)\n",
    "train_y0=np.array(train_y0)\n",
    "valc0=np.array(test_y0)\n",
    "\n",
    "train_X1=np.array(train_X1)\n",
    "vald1=np.array(test_X1)\n",
    "train_y1=np.array(train_y1)\n",
    "valc1=np.array(test_y1)\n",
    "\n",
    "train_X2=np.array(train_X2)\n",
    "vald2=np.array(test_X2)\n",
    "train_y2=np.array(train_y2)\n",
    "valc2=np.array(test_y2)\n",
    "\n",
    "train_X3=np.array(train_X3)\n",
    "vald3=np.array(test_X3)\n",
    "train_y3=np.array(train_y3)\n",
    "valc3=np.array(test_y3)\n",
    "\n",
    "train_X4=np.array(train_X4)\n",
    "vald4=np.array(test_X4)\n",
    "train_y4=np.array(train_y4)\n",
    "valc4=np.array(test_y4)\n",
    "\n",
    "train_data=np.concatenate((train_X0,train_X1,train_X1[0:160],train_X2,train_X2,train_X2,train_X2,train_X3,train_X3,train_X3,train_X3,train_X4,train_X4))\n",
    "train_label=np.concatenate((train_y0,train_y1,train_y1[0:160],train_y2,train_y2,train_y2,train_y2,train_y3,train_y3,train_y3,train_y3,train_y4,train_y4))\n",
    "val_data=np.concatenate((vald0,vald1,vald2,vald3,vald4))\n",
    "val_label=np.concatenate((valc0,valc1,valc2,valc3,valc4))\n",
    "\n",
    "print(train_data.shape, train_label.shape, val_data.shape, val_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "6C9l4x2IRwO-",
    "outputId": "e3c70161-ca9b-40f8-abfa-4031e879ad4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ((224, 224, 3), ()), types: (tf.uint8, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_label))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_label))\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "STOx8f3FRwPF"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 224 # All images will be resized to 160x160\n",
    "\n",
    "def format_example(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image/255) \n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "c8iQ46KCRwPN",
    "outputId": "4deba353-8aab-494a-f266-02da8b9e5c53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.MapDataset'>\n"
     ]
    }
   ],
   "source": [
    "train = train_dataset.map(format_example)\n",
    "validation = val_dataset.map(format_example)\n",
    "print(type(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ERE-0rT4RwPW"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "IMG_SHAPE = (IMG_SIZE,IMG_SIZE,3)\n",
    "\n",
    "train_batches = train.batch(BATCH_SIZE)\n",
    "validation_batches = validation.batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "o_4DQbHpRwPa",
    "outputId": "0aa9697c-01ff-41a0-d9d9-eb37d18e3188"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 224, 224, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for image_batch, label_batch in train_batches.take(1):\n",
    "    pass\n",
    "\n",
    "image_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_S9Z8aMRwPe"
   },
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.VGG16(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Nb-UBcF0RwPj",
    "outputId": "355c0df8-f4bc-415d-cd00-7701313b92aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 7, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eF6IbVidRwPp"
   },
   "outputs": [],
   "source": [
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HgU-AWeLwBzJ"
   },
   "outputs": [],
   "source": [
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 10\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable =  False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGC_b6mQRwPu"
   },
   "outputs": [],
   "source": [
    "p_layer = tf.keras.layers.Dense(512,activation='relu',kernel_regularizer=regularizers.l1_l2(l1=0.01,l2=0.01))\n",
    "p1_layer = tf.keras.layers.Dense(512,activation='relu',kernel_regularizer=regularizers.l1_l2(l1=0.01,l2=0.01))\n",
    "p2_layer = tf.keras.layers.Dense(512,activation='relu',kernel_regularizer=regularizers.l1_l2(l1=0.01,l2=0.01))\n",
    "p3_layer = tf.keras.layers.Dense(512,activation='relu',kernel_regularizer=regularizers.l1_l2(l1=0.01,l2=0.01))\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Y5ray0YvRwP1",
    "outputId": "98f03bee-4a8c-4d93-f841-f33e73b486fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5)\n"
     ]
    }
   ],
   "source": [
    "prediction_layer = tf.keras.layers.Dense(5,kernel_regularizer=regularizers.l1_l2(l1=0.01,l2=0.01))\n",
    "prediction_batch = prediction_layer(feature_batch_average)\n",
    "print(prediction_batch.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pxuiQ5NyRwP5"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  base_model,p1_layer,Dropout(0.6), BatchNormalization(momentum=0.9,epsilon=0.01),\n",
    "  p2_layer,Dropout(0.6), BatchNormalization(momentum=0.9,epsilon=0.01),\n",
    "  p3_layer,Dropout(0.6), BatchNormalization(momentum=0.9,epsilon=0.01),\n",
    "  global_average_layer,Dropout(0.6),BatchNormalization(momentum=0.9,epsilon=0.01),\n",
    "  p_layer,Dropout(0.5),BatchNormalization(momentum=0.9,epsilon=0.01),\n",
    "  Dropout(0.5),BatchNormalization(momentum=0.9,epsilon=0.01),prediction_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23IjejUDRwP9"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy','mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1TF5J3QRwP_"
   },
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRx4zfHiQtCh"
   },
   "outputs": [],
   "source": [
    "# initialize an our data augmenter as an \"empty\" image data generator\n",
    "aug = ImageDataGenerator()\n",
    "# check to see if we are applying \"on the fly\" data augmentation, and\n",
    "# if so, re-instantiate the object\n",
    "\n",
    "aug = ImageDataGenerator(\n",
    "\t\trotation_range=15,\n",
    "\t\tzoom_range=0.15,\n",
    "\t\twidth_shift_range=0.2,\n",
    "\t\theight_shift_range=0.2,\n",
    "\t\tshear_range=0.15,\n",
    "\t\thorizontal_flip=True,\n",
    "\t\tfill_mode=\"nearest\",\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "zrrhxuhmRwQF",
    "outputId": "a7ba1e64-1cbe-4405-d7f9-fdbdc0eff7de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "v-j6nc1ZRwQI",
    "outputId": "2d619f0b-aa36-4bcb-f0da-71dc9a892d4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 24ms/step - loss: 424.8987 - accuracy: 0.0000e+00 - mse: 0.0365\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 424.9797 - accuracy: 0.0000e+00 - mse: 0.2093\n"
     ]
    }
   ],
   "source": [
    "initial_epochs =10\n",
    "validation_steps=5\n",
    "\n",
    "loss_t,accuracy_t,rmse_t=model.evaluate(train_batches,steps=validation_steps) \n",
    "loss0,accuracy0,rmse0 = model.evaluate(validation_batches, steps = validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "SOGYJF0NRwQN",
    "outputId": "06ac67bc-a836-4215-a0f9-6d4edfdf15cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss: 424.98\n",
      "initial accuracy: 0.00\n"
     ]
    }
   ],
   "source": [
    "print(\"initial loss: {:.2f}\".format(loss0))\n",
    "print(\"initial accuracy: {:.2f}\".format(accuracy0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "7LeygDhsnRkz",
    "outputId": "aa316c84-d294-4e56-9651-48c02558c348"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.7008928571428571,\n",
       " 1: 0.6993318485523384,\n",
       " 2: 0.8820224719101124,\n",
       " 3: 0.8626373626373626,\n",
       " 4: 1.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_arr = compute_class_weight('balanced', np.unique(train_label), train_label)\n",
    "mwrr=(np.max(w_arr))\n",
    "class_weights = {0:(w_arr[0])/mwrr, 1:(w_arr[1])/mwrr, 2:(w_arr[2])/mwrr, 3:(w_arr[3])/mwrr, 4:(w_arr[4])/mwrr}\n",
    "class_weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0PGY1Q8URwQT",
    "outputId": "4dfa2711-4028-41c7-fd41-1c6361c626d2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:724: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "107/107 [==============================] - 31s 287ms/step - loss: 163.2383 - accuracy: 0.2817 - mse: 6.1021 - val_loss: 69.2285 - val_accuracy: 0.2587 - val_mse: 7.9970\n",
      "Epoch 2/100\n",
      "107/107 [==============================] - 30s 279ms/step - loss: 62.3105 - accuracy: 0.4417 - mse: 6.2264 - val_loss: 58.3021 - val_accuracy: 0.1990 - val_mse: 24.1463\n",
      "Epoch 3/100\n",
      "107/107 [==============================] - 30s 282ms/step - loss: 68.7747 - accuracy: 0.4917 - mse: 6.3882 - val_loss: 71.1115 - val_accuracy: 0.5572 - val_mse: 7.0598\n",
      "Epoch 4/100\n",
      "107/107 [==============================] - 30s 284ms/step - loss: 69.5067 - accuracy: 0.5117 - mse: 6.5697 - val_loss: 74.5564 - val_accuracy: 0.1990 - val_mse: 22.3844\n",
      "Epoch 5/100\n",
      "107/107 [==============================] - 30s 278ms/step - loss: 63.8259 - accuracy: 0.5113 - mse: 6.5213 - val_loss: 64.7237 - val_accuracy: 0.1990 - val_mse: 29.0691\n",
      "Epoch 6/100\n",
      "107/107 [==============================] - 30s 282ms/step - loss: 57.1311 - accuracy: 0.4908 - mse: 6.5740 - val_loss: 54.5783 - val_accuracy: 0.3682 - val_mse: 6.5575\n",
      "Epoch 7/100\n",
      "107/107 [==============================] - 30s 279ms/step - loss: 54.2093 - accuracy: 0.5233 - mse: 6.6194 - val_loss: 64.1779 - val_accuracy: 0.4975 - val_mse: 8.2942\n",
      "Epoch 8/100\n",
      "107/107 [==============================] - 30s 279ms/step - loss: 58.8607 - accuracy: 0.5277 - mse: 6.8141 - val_loss: 54.7480 - val_accuracy: 0.4080 - val_mse: 21.4922\n",
      "Epoch 9/100\n",
      "107/107 [==============================] - 30s 282ms/step - loss: 48.9480 - accuracy: 0.5312 - mse: 6.7943 - val_loss: 49.2206 - val_accuracy: 0.3781 - val_mse: 10.2949\n",
      "Epoch 10/100\n",
      "107/107 [==============================] - 30s 281ms/step - loss: 56.4785 - accuracy: 0.5284 - mse: 6.6183 - val_loss: 59.8417 - val_accuracy: 0.5522 - val_mse: 8.0472\n",
      "Epoch 11/100\n",
      "107/107 [==============================] - 30s 281ms/step - loss: 58.4146 - accuracy: 0.5388 - mse: 6.9729 - val_loss: 71.9790 - val_accuracy: 0.5174 - val_mse: 8.8950\n",
      "Epoch 12/100\n",
      "107/107 [==============================] - 30s 284ms/step - loss: 94.4361 - accuracy: 0.5346 - mse: 6.8084 - val_loss: 198.9267 - val_accuracy: 0.2239 - val_mse: 6603.6367\n",
      "Epoch 13/100\n",
      "107/107 [==============================] - 30s 281ms/step - loss: 126.8123 - accuracy: 0.5303 - mse: 6.6729 - val_loss: 113.8231 - val_accuracy: 0.4627 - val_mse: 8.1593\n",
      "Epoch 14/100\n",
      "107/107 [==============================] - 30s 282ms/step - loss: 98.5177 - accuracy: 0.5369 - mse: 6.6507 - val_loss: 14644.3379 - val_accuracy: 0.1990 - val_mse: 77144112.0000\n",
      "Epoch 15/100\n",
      "107/107 [==============================] - 30s 280ms/step - loss: 75.2281 - accuracy: 0.5478 - mse: 6.7188 - val_loss: 70.9742 - val_accuracy: 0.4776 - val_mse: 7.3028\n",
      "Epoch 16/100\n",
      "107/107 [==============================] - 30s 279ms/step - loss: 79.1844 - accuracy: 0.5334 - mse: 6.5404 - val_loss: 82.8512 - val_accuracy: 0.6119 - val_mse: 7.8361\n",
      "Epoch 17/100\n",
      "107/107 [==============================] - 30s 283ms/step - loss: 81.3646 - accuracy: 0.5318 - mse: 6.8261 - val_loss: 90.1184 - val_accuracy: 0.1990 - val_mse: 146.3625\n",
      "Epoch 18/100\n",
      "107/107 [==============================] - 30s 281ms/step - loss: 69.4169 - accuracy: 0.5293 - mse: 6.7237 - val_loss: 59.8115 - val_accuracy: 0.6219 - val_mse: 9.5179\n",
      "Epoch 19/100\n",
      "107/107 [==============================] - 30s 279ms/step - loss: 57.0759 - accuracy: 0.5054 - mse: 6.5930 - val_loss: 102.3824 - val_accuracy: 0.1990 - val_mse: 109.7319\n",
      "Epoch 20/100\n",
      "107/107 [==============================] - 30s 279ms/step - loss: 107.4340 - accuracy: 0.5500 - mse: 6.8863 - val_loss: 103.0726 - val_accuracy: 0.3881 - val_mse: 23.9868\n",
      "Epoch 21/100\n",
      "107/107 [==============================] - 30s 278ms/step - loss: 97.6500 - accuracy: 0.5236 - mse: 6.7969 - val_loss: 92.3943 - val_accuracy: 0.3930 - val_mse: 8.1694\n",
      "Epoch 22/100\n",
      "107/107 [==============================] - 30s 278ms/step - loss: 86.4123 - accuracy: 0.5318 - mse: 6.8232 - val_loss: 74.8715 - val_accuracy: 0.4826 - val_mse: 8.1826\n",
      "Epoch 23/100\n",
      "107/107 [==============================] - 30s 279ms/step - loss: 64.9908 - accuracy: 0.5440 - mse: 6.9519 - val_loss: 81.5531 - val_accuracy: 0.0697 - val_mse: 263.6053\n",
      "Epoch 24/100\n",
      "107/107 [==============================] - 29s 275ms/step - loss: 64.1992 - accuracy: 0.5107 - mse: 6.8961 - val_loss: 57.1409 - val_accuracy: 0.5721 - val_mse: 8.0769\n",
      "Epoch 25/100\n",
      "107/107 [==============================] - 30s 280ms/step - loss: 50.1745 - accuracy: 0.5186 - mse: 6.9505 - val_loss: 60.4577 - val_accuracy: 0.1990 - val_mse: 304.4312\n",
      "Epoch 26/100\n",
      "  5/107 [>.............................] - ETA: 22s - loss: 46.6879 - accuracy: 0.4600 - mse: 6.7844"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-1b111fd66413>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m H = model.fit(aug.flow(train_data,train_label,batch_size=30),\n\u001b[0;32m----> 2\u001b[0;31m \tvalidation_data=(val_data, val_label), steps_per_epoch=107, epochs=100,class_weight=class_weights)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    850\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#aug.fit(train_data)\n",
    "#aug_val.fit(val_data)\n",
    "H = model.fit(aug.flow(train_data,train_label,batch_size=30),\n",
    "\tvalidation_data=(val_data, val_label), steps_per_epoch=107, epochs=20,class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQIbQuY8RwQY"
   },
   "outputs": [],
   "source": [
    "acc = H.history['accuracy']\n",
    "val_acc = H.history['val_accuracy']\n",
    "\n",
    "loss = H.history['loss']\n",
    "val_loss = H.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "#plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "#plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRSkpZq_xuLr"
   },
   "outputs": [],
   "source": [
    "test_data = load_data(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rm8ilHbRRwQt"
   },
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
    "predictions_test = probability_model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gp4sep8k2XlV"
   },
   "outputs": [],
   "source": [
    "predictions_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQk0TtbXyOHB"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "pred = []\n",
    "p0 = 0\n",
    "p1 = 0\n",
    "p2 = 0\n",
    "p3 = 0\n",
    "p4 = 0\n",
    "for i in range(len(test_data)):\n",
    "    if np.argmax(predictions_test[i]) == 0: \n",
    "        p0 += 1\n",
    "        pred.append([1,0,0,0,0])\n",
    "    elif np.argmax(predictions_test[i]) == 1: \n",
    "        p1 += 1\n",
    "        pred.append([0,1,0,0,0])\n",
    "    elif np.argmax(predictions_test[i]) == 2: \n",
    "        p2 += 1\n",
    "        pred.append([0,0,1,0,0])\n",
    "    elif np.argmax(predictions_test[i]) == 3: \n",
    "        p3 += 1\n",
    "        pred.append([0,0,0,1,0])\n",
    "    elif np.argmax(predictions_test[i]) == 4: \n",
    "        p4 += 1\n",
    "        pred.append([0,0,0,0,1])\n",
    "\n",
    "df = pd.DataFrame(pred)\n",
    "df.to_csv(\"/content/drive/My Drive/C2/data/predict.csv\", index=False, header = False)\n",
    "with open(\"/content/drive/My Drive/C2/data/predict.csv\", \"w\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        for i in range(len(test_data)):\n",
    "            \n",
    "            writer.writerow(pred[i])\n",
    "print(p0,p1,p2,p3,p4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7ZwqK6qPjDe"
   },
   "outputs": [],
   "source": [
    "y_pred = np.zeros(len(val_label), dtype=np.int8)\n",
    "val_pred = probability_model.predict(val_data)\n",
    "\n",
    "for n in range(len(val_label)):\n",
    "  y_pred[n] = np.argmax(val_pred[n])\n",
    "  #print(y_pred[n],val_label[n])\n",
    "\n",
    "val = []\n",
    "v0 = 0\n",
    "v1 = 0\n",
    "v2 = 0\n",
    "v3 = 0\n",
    "v4 = 0\n",
    "for i in range(len(val_label)):\n",
    "    if val_label[i] == 0: \n",
    "        v0 += 1\n",
    "        val.append([1,0,0,0,0])\n",
    "    elif val_label[i] == 1: \n",
    "        v1 += 1\n",
    "        val.append([0,1,0,0,0])\n",
    "    elif val_label[i] == 2: \n",
    "        v2 += 1\n",
    "        val.append([0,0,1,0,0])\n",
    "    elif val_label[i] == 3: \n",
    "        v3 += 1\n",
    "        val.append([0,0,0,1,0])\n",
    "    elif val_label[i] == 4: \n",
    "        v4 += 1\n",
    "        val.append([0,0,0,0,1])\n",
    "\n",
    "val = np.array(val)\n",
    "#print(val_pred)\n",
    "#print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vDFIxZunPJ-1"
   },
   "outputs": [],
   "source": [
    "target_names = [0,1,2,3,4]\n",
    "plt.figure(1)\n",
    "c_matrix= confusion_matrix(val_label, y_pred,labels=[0,1,2,3,4,])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(np.array(c_matrix))\n",
    "               \n",
    "ax.set_xticks(np.arange(len(target_names)))\n",
    "ax.set_yticks(np.arange(len(target_names)))    \n",
    "\n",
    "ax.set_xticklabels(target_names)\n",
    "ax.set_yticklabels(target_names)\n",
    "\n",
    "ax.margins(1., y = 1)\n",
    "plt.setp(ax.get_xticklabels(), rotation=0, ha=\"right\",rotation_mode=\"anchor\")\n",
    "for i in range(len(target_names)):\n",
    "    for j in range(len(target_names)):\n",
    "        text = ax.text(j, i, c_matrix[i, j],\n",
    "                       ha='center', verticalalignment='center', color='w')\n",
    "fig.tight_layout(pad = 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2raurUE6SBNw"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(classification_report(val_label, y_pred, target_names = ['class 0', 'class 1', 'class 2', 'class 3', 'class 4']))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Soyabean_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
